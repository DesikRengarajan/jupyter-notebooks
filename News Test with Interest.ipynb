{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vowpalwabbit import pyvw\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VW tries to minimize loss/cost, therefore we will pass cost as -reward\n",
    "USER_LIKED_ARTICLE = -1.0\n",
    "USER_DISLIKED_ARTICLE = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"politics\", \"sports\", \"music\", \"food\", \"finance\", \"health\", \"camping\"]\n",
    "def get_cost(context,action):\n",
    "    action_max=np.argmax(context['interest'])\n",
    "    if context['user'] == \"Tom\":\n",
    "        if context['time_of_day'] == \"morning\" and action == 'politics':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        elif context['time_of_day'] == \"afternoon\" and action == actions[action_max]:\n",
    "            return USER_LIKED_ARTICLE\n",
    "        else:\n",
    "            return USER_DISLIKED_ARTICLE\n",
    "    elif context['user'] == \"Anna\":\n",
    "        if context['time_of_day'] == \"morning\" and action == 'sports':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        elif context['time_of_day'] == \"afternoon\" and action == actions[action_max]:\n",
    "            return USER_LIKED_ARTICLE\n",
    "        else:\n",
    "            return USER_DISLIKED_ARTICLE\n",
    "   \n",
    "#     action_i=np.argmax(context['interest'])\n",
    "#     if(action==actions[action_i]):        \n",
    "#         return -1\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function modifies (context, action, cost, probability) to VW friendly format\n",
    "def to_vw_example_format(context, actions, cb_label = None):\n",
    "    if cb_label is not None:\n",
    "        chosen_action, cost, prob = cb_label\n",
    "    example_string = \"\"\n",
    "    example_string += \"shared |User user={} time_of_day={} interest={} \\n\".format(context[\"user\"], context[\"time_of_day\"],context[\"interest\"])\n",
    "    for action in actions:\n",
    "        if cb_label is not None and action == chosen_action:\n",
    "            example_string += \"0:{}:{} \".format(cost, prob)\n",
    "        example_string += \"|Action article={} \\n\".format(action)\n",
    "    #Strip the last newline\n",
    "    return example_string[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared |User user=Tom time_of_day=morning interest=[0.13149200737184663, 0.3106442365623171, 0.0343433988294356, 0.16113591118151038, 0.1699663782735804, 0.4121432183781505, 0.12026216583121352] \n",
      "|Action article=politics \n",
      "|Action article=sports \n",
      "|Action article=music \n",
      "|Action article=food \n",
      "|Action article=finance \n",
      "|Action article=health \n",
      "|Action article=camping \n"
     ]
    }
   ],
   "source": [
    "actions = [\"politics\", \"sports\", \"music\", \"food\", \"finance\", \"health\", \"camping\"]\n",
    "context = {\"user\":\"Tom\",\"time_of_day\":\"morning\",\"interest\": list(np.random.random(len(actions)))}\n",
    "\n",
    "print(to_vw_example_format(context,actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a decision\n",
    "\n",
    "When we call VW we get a _pmf_, [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function), as the output. Since we are incorporating exploration into our strategy, VW will give us a list of probabilities over the set of actions. This means that the probability at a given index in the list corresponds to the likelihood of picking that specific action. In order to arrive at a decision/action, we will have to sample from this list.\n",
    "\n",
    "So, given a list `[0.7, 0.1, 0.1, 0.1]`, we would choose the first item with a 70% chance. `sample_custom_pmf` takes such a list and gives us the index it chose and what the probability of choosing that index was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_custom_pmf(pmf):\n",
    "    total = sum(pmf)\n",
    "    scale = 1/total\n",
    "    pmf = [x * scale for x in pmf]\n",
    "    draw = random.random()\n",
    "    sum_prob = 0.0\n",
    "    for index, prob in enumerate(pmf):\n",
    "        sum_prob += prob\n",
    "        if(sum_prob > draw):\n",
    "            return index, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all of the information we need to choose an action for a specific user and context. To use VW to achieve this, we will do the following:\n",
    "\n",
    "1. We convert our context and actions into the text format we need\n",
    "2. We pass this example to vw and get the pmf out\n",
    "3. Now, we sample this pmf to get what article we will end up showing\n",
    "4. Finally we return the article chosen, and the probability of choosing it (we are going to need the probability when we learn form this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(vw, context, actions):\n",
    "    vw_text_example = to_vw_example_format(context,actions)    \n",
    "    pmf = vw.predict(vw_text_example)\n",
    "    chosen_action_index, prob = sample_custom_pmf(pmf)\n",
    "    return actions[chosen_action_index], prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Simulation set up\n",
    "\n",
    "Now that we have done all of the setup work and know how to interface with VW, let's simulate the world of Tom and Anna. The scenario is they go to a website and are shown an article. Remember that the reward function allows us to define the worlds reaction to what VW recommends.\n",
    "\n",
    "\n",
    "We will choose between Tom and Anna uniformly at random and also choose their time of visit uniformly at random. You can think of this as us tossing a coin to choose between Tom and Anna (Anna if heads and Tom if tails) and another coin toss for choosing time of day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ['Tom', 'Anna']\n",
    "times_of_day = ['morning', 'afternoon']\n",
    "def choose_user(users):\n",
    "    return random.choice(users)\n",
    "\n",
    "def choose_time_of_day(times_of_day):\n",
    "    return random.choice(times_of_day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will instantiate a CB learner in VW and then simulate Tom and Anna's website visits `num_iterations` number of times. In each visit, we:\n",
    "\n",
    "1. Decide between Tom and Anna\n",
    "2. Decide time of day\n",
    "3. Pass context i.e. (user, time of day) to learner to get action i.e. article recommendation and probability of choosing action\n",
    "4. Receive reward i.e. see if user clicked or not. Remember that cost is just negative reward.\n",
    "5. Format context, action, probability, reward in VW format\n",
    "6. Learn from the example\n",
    "    - VW _reduces_ a CB problem to a cost sensitive multiclass classification problem.\n",
    "\n",
    "This is the same for every one of our simulations, so we define the process in the `run_simulation` function. The cost function must be supplied as this is essentially us simulating how the world works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(vw, num_iterations, users, times_of_day, actions, cost_function, do_learn = True):\n",
    "    cost_sum = 0.\n",
    "    ctr = []\n",
    "\n",
    "    for i in range(1, num_iterations+1):\n",
    "        # 1. In each simulation choose a user\n",
    "        user = choose_user(users)\n",
    "        # 2. Choose time of day for a given user\n",
    "        time_of_day = choose_time_of_day(times_of_day)\n",
    "\n",
    "        # 3. Pass context to vw to get an action\n",
    "        context = {'user': user, 'time_of_day': time_of_day,\"interest\": list(np.random.random(len(actions)))}\n",
    "        action, prob = get_action(vw, context, actions)\n",
    "\n",
    "        # 4. Get cost of the action we chose\n",
    "        cost = cost_function(context, action)\n",
    "        cost_sum += cost\n",
    "\n",
    "        if do_learn:\n",
    "            # 5. Inform VW of what happened so we can learn from it\n",
    "            vw_format = vw.parse(to_vw_example_format(context, actions, (action, cost, prob)),pyvw.vw.lContextualBandit)            \n",
    "            # 6. Learn\n",
    "            vw.learn(vw_format)\n",
    "\n",
    "        # We negate this so that on the plot instead of minimizing cost, we are maximizing reward\n",
    "        ctr.append(-1*cost_sum/i)\n",
    "\n",
    "    return ctr,cost_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We want to be able to visualize what is occurring, so we are going to plot the click through rate over each iteration of the simulation. If VW is showing actions the get rewards the ctr will be higher. Below is a little utility function to make showing the plot easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ctr(num_iterations, ctr):\n",
    "    plt.plot(range(1,num_iterations+1), ctr)\n",
    "    plt.xlabel('num_iterations', fontsize=14)\n",
    "    plt.ylabel('ctr', fontsize=14)\n",
    "    plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1\n",
    "\n",
    "We will use the first reward function `get_cost` and assume that Tom and Anna do not change their preferences over time and see what happens to user engagement as we learn. We will also see what happens when there is no learning. We will use the \"no learning\" case as our baseline to compare to.\n",
    "\n",
    "### With learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate learner in VW\n",
    "learning_rates=[10,1,1e-1,1e-2,1e-4,1e-5,1e-6]\n",
    "power_t_rates=[0,1]\n",
    "regular=[1e-9,1e-6,1e-5,1e-3,1e-2]\n",
    "num_iterations = 5000\n",
    "values={}\n",
    "iter=0\n",
    "for lr in learning_rates:\n",
    "    for pwr in power_t_rates:\n",
    "        for reg in regular:\n",
    "            vw = pyvw.vw(\"--cb_explore_adf -q UA --quiet --epsilon 0.3\"\n",
    "                         +str(' -l ')+str(lr)+str(' -r ')+str(reg)+str(' -t ')+str(pwr))\n",
    "            ctr,cost_sum = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost)    \n",
    "            values[lr,pwr,reg]=-1*cost_sum\n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "# print('Cumulative Reward='+str(-1*cost_sum))\n",
    "# plot_ctr(num_iterations, ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 0.01)\n",
      "765.0\n"
     ]
    }
   ],
   "source": [
    "max_param=max(values,key=values.get)\n",
    "print(max_param)\n",
    "print(values[max_param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: interactions\n",
    "You'll notice in the arguments we supply to VW, **we include `-q UA`**. This is telling VW to create additional features which are the features in the (U)ser namespace and (A)ction namespaces multiplied together. This allows us to learn the interaction between when certain actions are good in certain times of days and for particular users. If we didn't do that, the learning wouldn't really work. We can see that in action below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
